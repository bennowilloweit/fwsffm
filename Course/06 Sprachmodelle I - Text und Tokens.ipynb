{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c7ac81-8700-4030-b069-e87a94162c4c",
   "metadata": {},
   "source": [
    "# Sprachmodelle I - Text und Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447464a9-4b64-4626-a59a-fd74e6d44205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = \"{:,.4f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13330bf6-884c-43ce-b65a-a8b8350d9668",
   "metadata": {},
   "source": [
    "Wir ladene einen ersten Text aus einer Datei und geben ihn aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce39ed2-78eb-4dd6-9f4f-3e4101872f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('txt/zauberlehrling.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8886fb15-9ca8-433c-b783-c8d0f83915fd",
   "metadata": {},
   "source": [
    "## Von Text zu Zahlen und wieder zurück"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3664d5-4c7c-4d97-97d6-7172144cc83e",
   "metadata": {},
   "source": [
    "Um Texte im Computer verarbeiten zu können, müssen wir sie irgendwie auf Zahlen abbilden. Dazu brauchen wir irgendeine Vorschrift. Eine einfache Vorschrift, die auch traditionell in Computern verwendet wird, ist es jedem Buchstaben und jedem Zeichen eine Zahl zuzuordnen. Bei der Entwicklung der Sprachmodelle stellte sich jedoch heraus, dass das zusammenfassen von einzelnen Buchstaben und Zeichen zu besseren Ergebnissen führt. Diese zusammengefassten Zeichen nennt man Token. Der Einfachheit halber haben wir die Buchstaben eines Wortes immer als einen Token aufgefasst und jedes Sonderzeichen bzw. jeden Zeilenumbruch als einen eigenen Token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05c15d-12c8-4f90-845c-ff5049286fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fws.tokenizer import FWSTokenizer\n",
    "tokenizer = FWSTokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea85406b-c764-4d22-95b9-fb9a16b25364",
   "metadata": {},
   "source": [
    "Zunächst machen wir den Computer bekannt mit den allen Texten, die er beherrschen soll. Dazu erzeugen wir einen Tokenizer und füttern ihn erstmal nur mit dem Gedicht 'Der Zauberlehrling'. Der Tokenizer zerlegt das Gedicht in Token und merkt sich, welche Token vorkamen. Die ersten 15 sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad0cff-6a95-4603-944e-70e7164ca843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data={'token':tokenizer.vocab_str(), 'output':tokenizer.decode_list(tokenizer.vocab_int())}, \n",
    "    index=tokenizer.vocab_int())\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf65048-d5d3-46ae-ac2d-b26179334afa",
   "metadata": {},
   "source": [
    "Insgesamt hat der Tokenizer so viele unterschiedliche Token gefunden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb9775-84b2-4688-a6b2-2f90e31b81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.vsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1e162-ae62-4c20-84e6-50be68efc214",
   "metadata": {
    "tags": []
   },
   "source": [
    "Der Tokenizer kann aus Texten Zahlen machen und aus Zahlen wieder Texte. Aber er kann nur mit den Texten umgehen, die er auch kennengelernt hat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653edcdc-a73e-4908-bc14-b53cbdd6343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuer_text = \"Ich checke das nicht\"\n",
    "neue_idx_seq = tokenizer.encode(tokenizer.tokenize(neuer_text))\n",
    "print(neue_idx_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a761f1-7d17-4804-9332-789ac695baca",
   "metadata": {},
   "source": [
    "Wenn wir die Zahlen wieder in Buchstaben verwandeln fällt es auf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d807cd-2602-4bd2-a53d-e75a3ceb2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(neue_idx_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9044628-2882-41bf-a8b6-83c49e085735",
   "metadata": {},
   "source": [
    "#### Aufgabe 1:\n",
    "\n",
    "##### 0,5+0,5 = 1 Punkt\n",
    "\n",
    "- F: Benennen Sie einen Token im Text 'Der Zauberlehrling', der mehr als einmal vorkommt\n",
    "- A:\n",
    "- F: Warum kommt beim zurückverwandeln von 'Ich checke das nicht' nur noch 'das nicht' raus?\n",
    "- A:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256fb03f-5de1-486a-bc35-a18f76e15b80",
   "metadata": {
    "tags": []
   },
   "source": [
    "Zu wissen welche Token es gibt, ist auch hilfreich um aus den bekannten Token neue Texte erzeugen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860b88d-d9d1-4ca6-9dcc-56249e465f5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Auf der Suche nach einem Modell, das Texte schreibt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6555f3-2e2c-4d9e-88e4-6f176a25b009",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Versuch 1\n",
    "\n",
    "Wir werfen alle Token in einen Sack, schütteln kräftig und ziehen zufällig 32 Stück hinterinander heraus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad2320-2afc-48ec-a000-73f10d9a58e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "gen_idx_seq = rng.integers(0, tokenizer.vsize - 1, size=32)\n",
    "print(tokenizer.decode(gen_idx_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d06c7-bdd3-484c-94cc-82ee95067622",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vorbereitung Versuch 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab78014-aa05-4ddf-af72-6fdd13a025c6",
   "metadata": {},
   "source": [
    "Für den zweiten Versuch zählen wir, wievele Token im Gedicht insgesamt vorkamen und wie häufig jeder einzelne vorkaum. Daraus können wir die relative Häufigkeit berechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe67f8c-c178-4390-97b7-c684b8ca86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tokenizer.tokenize(text)\n",
    "print(len(toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f335b94-e34f-4e92-b961-514616ebce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.zeros(tokenizer.vsize, dtype=torch.int32)\n",
    "for tok in toks:\n",
    "    c[tokenizer.idx(tok)] += 1\n",
    "\n",
    "df['count'] = c \n",
    "df['frq'] = df['count'] / len(toks)\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78eddf-8270-482a-8b82-98336b3ecf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values('frq', ascending=False)\n",
    "df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95726dea-b5c0-4eea-baff-8cc89d15e658",
   "metadata": {
    "tags": []
   },
   "source": [
    "Die relative Häufigkeit summiert sich auf 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f398c-2769-49f3-82f0-c564e1c10070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['frq'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c05508-3c44-4111-8b3a-ff46cd2a61cf",
   "metadata": {},
   "source": [
    "### Versuch 2\n",
    "\n",
    "Jetzt werfen wir wieder alle Token die wir kennen in einen Sack, schütteln Kräftig und ziehen mit einer magischen Hand jetzt wieder zufällig, aber berücksichtigen dabei die relative Häufigkeit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740cb8e-5c86-478f-a454-14e7c1fad4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_idx_seq = rng.choice(tokenizer.vocab_int(), size=32, replace=True, p=df['frq'].to_numpy())\n",
    "print(tokenizer.decode(gen_idx_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b3a4c-b6f4-4f0e-9c9b-44bfc41a0d81",
   "metadata": {},
   "source": [
    "#### Aufgabe 2\n",
    "\n",
    "##### 1+1+1 = 3 Punkte\n",
    "\n",
    "Führen Sie die Versuche 1 und 2 mehrfach aus und Vergleichen Sie die Ergebnisse.\n",
    "\n",
    "- F: Was sind die größten Unterschiede zwischen den Ergebnissen?\n",
    "- A:\n",
    "- F: Welcher Versuch produziert Ergebnisse, die näher an einem Gedicht sind?\n",
    "- A:\n",
    "- F: Warum ergibt keiner der Texte Sinn?\n",
    "- A:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8580c6-6c90-40df-9e1a-2cf641998207",
   "metadata": {},
   "source": [
    "### Vorbereitung Versuch 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf9771-8ca6-4ff0-992b-eaf21073e4d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Für den dritten Versuch zählen wir jetzt nicht nur, wie oft ein Token vorkommt, sondern auch, welcher Token wie oft davor steht. Die Zahlen sammeln wir in einer Matrix. Das ist eine Tabelle, in der jede Zeile und jede Spalte einem Token entspricht, und jede Zelle wie oft die beiden Token aus Zeile und Spalte im Text hintereinander vorkommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2ef50-c2d1-41ee-bd0c-23d9bb7dbef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((tokenizer.vsize, tokenizer.vsize), dtype=torch.int32)\n",
    "toks = tokenizer.tokenize(text)\n",
    "for t1, t2 in zip(toks, toks[1:]):\n",
    "    ix1 = tokenizer.idx(t1)\n",
    "    ix2 = tokenizer.idx(t2)\n",
    "    N[ix1, ix2] += 1\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a4a84e-7c5b-4365-9cf8-c870ff6c81ec",
   "metadata": {},
   "source": [
    "Damit wir die Matrix besser lesen können, stellen wir sie nochmal mit Überschriften für die Zeilen und Spalten dar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f3ee3-bea0-4777-a669-aee7ef513223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "bidf = pd.DataFrame(N.numpy(), columns=tokenizer.vocab_str(), index=tokenizer.vocab_str())\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "bidf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd3761e-02ec-4cc0-a793-b1d5e63a698f",
   "metadata": {},
   "source": [
    "Weil die Tabelle recht groß ist, ist sie hier nicht vollständig dargestellt. Wir können aber herausfinden welchen Wert eine Zelle in der Tabelle hat. So können wir herausfinden, wie oft die Token 'ich' und 'bin' hintereinander im Text vorkommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747a6fe-5558-4ce2-8b0b-6ddb14050e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = 'ich'\n",
    "next_tok = 'bin'\n",
    "N[tokenizer.idx(tok), tokenizer.idx(next_tok)].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f971995-d0d2-4158-83a9-f57ab8fc98e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Wir können auch herausfinden, wie oft ein bestimmter Token im Text auftaucht, indem wir die Zeile (oder Spalte) dieses Tokens aufsummieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c19bb-f6e8-4226-bf62-4f69d413cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = 'ich'\n",
    "N[tokenizer.idx(tok), :].sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91442122-53eb-4a8a-85bd-b41d2cb44234",
   "metadata": {
    "tags": []
   },
   "source": [
    "Wir können auch eine ganze Tabellenzeile ausgeben, beispielsweise um zu sehen, welche anderen Token wie oft auf 'ich' folgen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee8776-d5d6-456b-a1ff-17ee4a2d500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = 'ich'\n",
    "N[tokenizer.idx(tok), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ee493-aaba-4b91-835d-d842e9f20814",
   "metadata": {},
   "source": [
    "Da das schwer zu lesen ist, können wir uns mit ein bisschen mehr code auch die Liste aller Token ausgeben lassen, die auf 'ich' folgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa4229-967b-417c-870f-bfb288595bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = 'ich'\n",
    "ich_next = [tokenizer.tok(idx) for idx,cnt in enumerate(N[tokenizer.idx(tok), :].tolist()) if cnt > 0]\n",
    "print(ich_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5fa15a-ee66-419b-8a13-65968a0462c9",
   "metadata": {},
   "source": [
    "Erinnern wir uns an Versuch 2, wo wir die relative Häufigkeit der einzelnen Token berechnet haben. Mit Hilfe der Matrix die wir oben erzeugt haben, können wir jetzt die relative Häufigkeit für jeden Token angeben, der auf einen anderen folgt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe10b8-aa88-4c97-b916-3ba79185c7d5",
   "metadata": {},
   "source": [
    "#### Aufgabe 3\n",
    "\n",
    "##### 0,5+0,5+0,5+0,5 = 2 Punkte\n",
    "\n",
    "In jeder Zeile der Tabelle oben lässt sich ablesen, welcher Token wie oft auf den Token am Beginn dieser Zeile folgt. \n",
    "\n",
    "Beispiel: In der 8. Zeile steht ganz links 'Ach' und es lässt sich ablesen, dass in dieser Zeile eine 4 in der Spalte \\<CM\\> und eine 2 in der Spalte \\<EM\\> steht. Das Bedeutet im Text folgt auf 'Ach' viermal ein Komma und zweimal Ausrufezeichen.\n",
    "- F: Wie viele mögliche Token folgen auf 'und'?\n",
    "- A:\n",
    "- F: Gibt es einen Token der zweimal hintereinander vorkommt? Wo in der Tabelle können Sie das ablesen? \n",
    "- A:\n",
    "- F: Wo kann man ablesen, welcher Token vor einem Zeilenumbruch (Token: \\<NL\\>) stehen?\n",
    "- A:\n",
    "- F: Wie kann man ablesen, wie oft ein bestimmter Token insgesamt vorkommt?\n",
    "- A:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d6542-281c-4f99-afde-53724f0884fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ... auf der Zielgeraden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1bfee3-b36c-4cfd-97e1-4dbffbf3e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = N.float()\n",
    "P /= P.sum(1, keepdim=True)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d5c5f-88a5-47ed-84ee-f73be59f4548",
   "metadata": {},
   "source": [
    "Wir können diese Matrix der relativen Häufigkeiten auch als Bild darstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12eab2f-7e2f-4fd6-8601-25caec1b3470",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361679c9-cc95-4191-8c83-f940b286aa22",
   "metadata": {},
   "source": [
    "### Versuch 3\n",
    "\n",
    "Im dritten Versuch gehen wir schrittweise vor. Wir starten mit einem Token, mit dem Üblicherweise ein Gedicht anfängt, nämlich einer neuen Zeile. Jetzt werfen wir alle Token, die auf die neue Zeile folgen in einen Sack und schütteln kräftig. Dann ziehen mit einer magischen Hand jetzt wieder zufällig, aber berücksichtigen dabei die relative Häufigkeit der Token, die unserem Startoken folgen. Wir haben jetzt einen neuen Token und werfen wieder alle Token, die auf diesen neuen Token folgen in einen Sack... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f286a7d-3bb4-413f-a85c-8d4a4a0c5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = '<NL>'\n",
    "max_tokens = 32\n",
    "gen_idx_seq = [tokenizer.idx(start_token)]\n",
    "while True:\n",
    "    probs = P[gen_idx_seq[-1]]\n",
    "    idx = torch.multinomial(probs, num_samples=1).item()\n",
    "    gen_idx_seq.append(idx)\n",
    "    if len(gen_idx_seq) >= max_tokens:\n",
    "        break\n",
    "\n",
    "print(tokenizer.decode(gen_idx_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766dc79-c295-4fb2-9a39-1ea20b3b83a1",
   "metadata": {},
   "source": [
    "#### Aufgabe 4\n",
    "\n",
    "##### 1+1+2 = 4 Punkte\n",
    "\n",
    "- F: Was sieht man auf dem Bild oben?\n",
    "- A:\n",
    "- F: Wiederholen Sie den dritten Versuch ein paar mal. Was beobachten sie im Vergleich zu, zweiten Versuch?\n",
    "- A:\n",
    "- F: In Versuch 3 haben wir unser erstes Sprachmodell erstellt. Wie würden Sie es bezeichnen und was sind die Parameter des Modells? \n",
    "- A:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccffdd8-d4ec-4ee6-8bf1-3bb18a4576d9",
   "metadata": {},
   "source": [
    "## Wie messen wir wie gut unser Modell ist?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf8c9d-349f-47d8-ac4c-d03e1c5d86be",
   "metadata": {},
   "source": [
    "Wie groß ist die durchschnittliche realtive Häufigekit (\"Wahrscheinlichkeit\") eines Tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cbe5e5-112d-4c6d-a31e-274a4ed71820",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/tokenizer.vsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ac51b-d9c7-4d7f-85b7-e2fa9ccee202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def berechne_fehler(tokenizer, text, pMatrix):\n",
    "    data = []\n",
    "    toks = tokenizer.tokenize(text)\n",
    "    likelihood = 1\n",
    "    log_likelihood = 0.0\n",
    "    print_pairs = 10\n",
    "    count = 0\n",
    "    for t1, t2 in zip(toks, toks[1:]):\n",
    "        ix1 = tokenizer.idx(t1)\n",
    "        ix2 = tokenizer.idx(t2)\n",
    "        p = pMatrix[ix1, ix2]\n",
    "        logp = torch.log(p)\n",
    "        likelihood *= p\n",
    "        log_likelihood += logp\n",
    "        count += 1\n",
    "        data.append({'Token 1':t1, 'Token 2':t2, 'Likelihood':p.item(), 'Log Likelihood':logp.item(), 'Negative Log Likelihood':-logp.item()})\n",
    "        \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c32c25-97e2-4dfb-a60b-7798d668bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf = berechne_fehler(tokenizer, text, P)\n",
    "edf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d869b-57f4-4d3a-91fc-af53cb997f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Likelihood: {edf['Likelihood'].product()}\") \n",
    "print(f\"Log Likelihood: {edf['Log Likelihood'].sum()}\")\n",
    "log_likelihood = edf['Likelihood'].product()\n",
    "neg_log_likelihood = -log_likelihood\n",
    "print(f\"Negative Log Likelihood: {edf['Negative Log Likelihood'].sum()}\")\n",
    "neg_log_likelihood = -log_likelihood\n",
    "print(f\"Average Negative Log Likelihood (Fehler): {edf['Negative Log Likelihood'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b65e2-1283-44a5-929b-b113fbbe00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf[edf['Likelihood'] == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d31790-0854-46d1-a9f7-9ed426be2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf = berechne_fehler(tokenizer, \"ich checke das nicht\", P)\n",
    "edf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2a0bf-71c1-4dcb-9ed1-f7969c96d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "P2 = (N + 1).float()\n",
    "P2 /= P2.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04789342-6775-47bf-8a56-a98d136d7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf = berechne_fehler(tokenizer, \"ich checke das nicht\", P2)\n",
    "edf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376e0da-a16a-426e-b878-f6aed263feb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
